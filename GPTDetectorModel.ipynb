{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7921ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load dataset\n",
    "testdata = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "testdata.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9d926",
   "metadata": {},
   "source": [
    "This shows the dataset is in the format required to begin feature extraction. The format for the answer column is in the order\n",
    "of human response followed by GPT response and continues for each of the questions in the dataset. The results column gives a \n",
    "binary value to correlate human vs GPT responses.\n",
    "\n",
    "Result Column:\n",
    "    0 - Human Response\n",
    "    1 - GPT Generated Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your .csv file into a DataFrame\n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "questions = df['question']\n",
    "answers = df['answer']\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit Tokenizer on questions and answers\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "# Tokenize questions and answers\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc3ea3",
   "metadata": {},
   "source": [
    "Testing the Tokenizer for tensorflow preprocessing is successful in generating a tokenized version of each question and human and AI generated response. The next step in this process is to decide what features to use, and how to accurately perform the feature extraction of these tokenized responses.\n",
    "\n",
    "Possible Candidates for Features:\n",
    "1. Word Embeddings with padded sequences\n",
    "2. BERT pre-trained model \n",
    "3. Bag-of-Words Representation\n",
    "4. N-gram features\n",
    "5. Statistical features such as sequence length, average word length, etc.\n",
    "\n",
    "From the research I have done so far with the project, I believe using both word embeddings and BERT to capture a contextual understanding, semantic similarity, sentence-level representation, and handling of variablity in the responses should be sufficient in order to train a model to detect, with a reasonable accuracy, if a response is human or GPT generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fe30d",
   "metadata": {},
   "source": [
    "In order to use the tokenized responses in the model, the sequences need to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_len = int(vocab_size/30)\n",
    "padded_questions = tf.keras.preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=max_len, padding='post')\n",
    "padded_answers = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=max_len, padding='post')\n",
    "\n",
    "# Define input_data and labels based on padded_questions and padded_answers\n",
    "input_data = np.concatenate((padded_questions, padded_answers), axis=1)\n",
    "labels = df['result'].values\n",
    "# Define a generator function to yield batches of data\n",
    "#def data_generator(padded_questions, padded_answers, labels, batch_size):\n",
    " #   num_samples = len(labels)\n",
    " #   indices = np.arange(num_samples)\n",
    "  #  np.random.shuffle(indices)  # Shuffle indices for randomness\n",
    "\n",
    " #   while True:\n",
    "   #     for start_idx in range(0, num_samples, batch_size):\n",
    "   #         end_idx = min(start_idx + batch_size, num_samples)\n",
    "   #         batch_indices = indices[start_idx:end_idx]\n",
    "    #        yield {'question_input': padded_questions[batch_indices], 'answer_input': padded_answers[batch_indices]}, labels[batch_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ec5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define input layers for questions and answers separately\n",
    "question_input = Input(shape=(max_len,), dtype='int32', name='question_input')\n",
    "answer_input = Input(shape=(max_len,), dtype='int32', name='answer_input')\n",
    "\n",
    "# Embedding layers for questions and answers\n",
    "embedding_dim = 50\n",
    "question_embedding = Embedding(vocab_size, embedding_dim, input_length=max_len)(question_input)\n",
    "answer_embedding = Embedding(vocab_size, embedding_dim, input_length=max_len)(answer_input)\n",
    "\n",
    "# LSTM layers for questions and answers\n",
    "question_lstm = LSTM(32)(question_embedding)\n",
    "answer_lstm = LSTM(32)(answer_embedding)\n",
    "\n",
    "# Concatenate the outputs of LSTM layers\n",
    "concatenated = Concatenate()([question_lstm, answer_lstm])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=[question_input, answer_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define batch size and epochs\n",
    "batch_size = 128  # Increased batch size for better GPU utilization\n",
    "epochs = 3\n",
    "\n",
    "# Train the model with early stopping\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)]  # Stop training if validation loss doesn't improve\n",
    "model.fit({'question_input': padded_questions, 'answer_input': padded_answers}, labels, \n",
    "          batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca19bd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
